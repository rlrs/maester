from maester.models.deepseek.model import DeepSeekModel, DeepSeekModelArgs
from maester.models.deepseek.optimizer import build_deepseek_optimizers

__all__ = ["DeepSeekModel", "DeepSeekModelArgs", "build_deepseek_optimizers"]

deepseek_configs = {
    "debug": DeepSeekModelArgs(
        max_batch_size=32,
        vocab_size=102400,
        dim=512,
        inter_dim=2048,
        moe_inter_dim=512,
        n_layers=8,
        n_dense_layers=1,
        n_heads=16,
        n_routed_experts=8,
        n_shared_experts=2,
        n_activated_experts=3,
        route_scale=1.0,
        q_lora_rank=0,
        kv_lora_rank=512,
        qk_nope_head_dim=128,
        qk_rope_head_dim=64,
        v_head_dim=128,
        mscale=0.70,
    ),
    "debug_flex": DeepSeekModelArgs(
        vocab_size=2000,
        dim=256,
        inter_dim=1024,
        moe_inter_dim=256,
        n_layers=3,
        n_dense_layers=1,
        n_heads=16,
        n_routed_experts=8,
        n_shared_experts=2,
        n_activated_experts=3,
        route_scale=1.0,
        q_lora_rank=0,
        kv_lora_rank=512,
        qk_nope_head_dim=128,
        qk_rope_head_dim=64,
        v_head_dim=128,
        mscale=0.70,
        use_flex_attn=True,
        attn_mask_type="block_causal",
    ),
    "16B": DeepSeekModelArgs(
        vocab_size=102400,
        dim=2048,
        inter_dim=10944,
        moe_inter_dim=1408,
        n_layers=27,
        n_dense_layers=1,
        n_heads=16,
        n_routed_experts=64,
        n_shared_experts=2,
        n_activated_experts=6,
        route_scale=1.0,
        q_lora_rank=0,
        kv_lora_rank=512,
        qk_nope_head_dim=128,
        qk_rope_head_dim=64,
        v_head_dim=128,
        mscale=0.70,
        use_flex_attn=True,
        attn_mask_type="causal",
    ),
    "45B_custom": DeepSeekModelArgs(
        vocab_size=102400,
        dim=3072,
        inter_dim=8192,
        moe_inter_dim=1408,
        n_layers=20,
        n_dense_layers=1,
        n_heads=128,
        n_routed_experts=160,
        n_shared_experts=2,
        n_activated_experts=6,
        route_scale=16.0,
        q_lora_rank=1536,
        kv_lora_rank=512,
        qk_nope_head_dim=128,
        qk_rope_head_dim=64,
        v_head_dim=128,
        mscale=0.70,
        use_flex_attn=True,
        attn_mask_type="causal",
    ),
    # TODO: check correctness of configs below here
    "236B": DeepSeekModelArgs(
        vocab_size=102400,
        dim=5120,
        inter_dim=12288,
        moe_inter_dim=1536,
        n_layers=60,
        n_dense_layers=1,
        n_heads=128,
        n_routed_experts=160,
        n_shared_experts=2,
        n_activated_experts=6,
        route_scale=16.0,
        q_lora_rank=1536,
        kv_lora_rank=512,
        qk_nope_head_dim=128,
        qk_rope_head_dim=64,
        v_head_dim=128,
        mscale=0.70,
        use_flex_attn=True,
        attn_mask_type="causal",
    ),
    "685B": DeepSeekModelArgs(
        vocab_size=129280,
        dim=7168,
        inter_dim=18432,
        moe_inter_dim=2048,
        n_layers=61,
        n_dense_layers=3,
        n_heads=128,
        n_routed_experts=256,
        n_shared_experts=1,
        n_activated_experts=8,
        route_scale=2.5,
        q_lora_rank=1536,
        kv_lora_rank=512,
        qk_nope_head_dim=128,
        qk_rope_head_dim=64,
        v_head_dim=128,
        mscale=1.0,
        use_flex_attn=True,
        attn_mask_type="causal",
    ),
}