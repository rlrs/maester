from maester.models.deepseek.model import DeepSeekModel, DeepSeekModelArgs
from maester.models.moe import MoEArgs
from maester.models.deepseek.optimizer import build_deepseek_optimizers

__all__ = ["DeepSeekModel", "DeepSeekModelArgs", "build_deepseek_optimizers"]

deepseek_configs = {
    "debug": DeepSeekModelArgs(
        max_batch_size=32,
        vocab_size=129280,
        dim=512,
        inter_dim=2048,
        moe_intermediate_size=512,
        n_layers=8,
        first_k_dense_replace=1,
        n_heads=16,
        moe_args=MoEArgs(
            num_experts=8,
            num_shared_experts=2,
            top_k=3,
            score_func="softmax",
            route_scale=1.0,
            use_grouped_mm=True,
            load_balance_coeff=None,
        ),
        q_lora_rank=0,
        kv_lora_rank=512,
        qk_nope_head_dim=128,
        qk_rope_head_dim=64,
        v_head_dim=128,
        mscale=0.70,
    ),
    "debug_flex": DeepSeekModelArgs(
        vocab_size=2000,
        dim=256,
        inter_dim=1024,
        moe_intermediate_size=256,
        n_layers=3,
        first_k_dense_replace=1,
        n_heads=16,
        moe_args=MoEArgs(
            num_experts=8,
            num_shared_experts=2,
            top_k=3,
            score_func="softmax",
            route_scale=1.0,
            use_grouped_mm=True,
            load_balance_coeff=None,
        ),
        q_lora_rank=0,
        kv_lora_rank=512,
        qk_nope_head_dim=128,
        qk_rope_head_dim=64,
        v_head_dim=128,
        mscale=0.70,
        use_flex_attn=True,
        attn_mask_type="causal",
    ),
    "16B": DeepSeekModelArgs(
        vocab_size=102400,
        dim=2048,
        inter_dim=10944,
        moe_intermediate_size=1408,
        n_layers=27,
        first_k_dense_replace=1,
        n_heads=16,
        moe_args=MoEArgs(
            num_experts=64,
            num_shared_experts=2,
            top_k=6,
            score_func="softmax",
            route_scale=1.0,
            use_grouped_mm=True,
            load_balance_coeff=None,
        ),
        q_lora_rank=0,
        kv_lora_rank=512,
        qk_nope_head_dim=128,
        qk_rope_head_dim=64,
        v_head_dim=128,
        mscale=0.70,
        use_flex_attn=True,
        attn_mask_type="causal",
    ),
    "45B_custom": DeepSeekModelArgs(
        vocab_size=102400,
        dim=3072,
        inter_dim=8192,
        moe_intermediate_size=1408,
        n_layers=20,
        first_k_dense_replace=1,
        n_heads=128,
        moe_args=MoEArgs(
            num_experts=160,
            num_shared_experts=2,
            top_k=6,
            score_func="softmax",
            route_scale=16.0,
            use_grouped_mm=True,
            load_balance_coeff=None,
        ),
        q_lora_rank=1536,
        kv_lora_rank=512,
        qk_nope_head_dim=128,
        qk_rope_head_dim=64,
        v_head_dim=128,
        mscale=0.70,
        use_flex_attn=True,
        attn_mask_type="causal",
    ),
    # TODO: check correctness of configs below here
    "236B": DeepSeekModelArgs(
        vocab_size=102400,
        dim=5120,
        inter_dim=12288,
        moe_intermediate_size=1536,
        n_layers=60,
        first_k_dense_replace=1,
        n_heads=128,
        moe_args=MoEArgs(
            num_experts=160,
            num_shared_experts=2,
            top_k=6,
            score_func="softmax",
            route_scale=16.0,
            use_grouped_mm=True,
            load_balance_coeff=None,
        ),
        q_lora_rank=1536,
        kv_lora_rank=512,
        qk_nope_head_dim=128,
        qk_rope_head_dim=64,
        v_head_dim=128,
        mscale=0.70,
        use_flex_attn=True,
        attn_mask_type="causal",
    ),
    "685B": DeepSeekModelArgs(
        vocab_size=129280,
        dim=7168,
        inter_dim=18432,
        moe_intermediate_size=2048,
        n_layers=61,
        first_k_dense_replace=3,
        n_heads=128,
        moe_args=MoEArgs(
            num_experts=256,
            num_shared_experts=1,
            top_k=8,
            score_func="softmax",
            route_scale=2.5,
            use_grouped_mm=True,
            load_balance_coeff=None,
        ),
        q_lora_rank=1536,
        kv_lora_rank=512,
        qk_nope_head_dim=128,
        qk_rope_head_dim=64,
        v_head_dim=128,
        mscale=1.0,
        use_flex_attn=True,
        attn_mask_type="causal",
    ),
}