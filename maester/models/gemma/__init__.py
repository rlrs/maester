from maester.models.gemma.model import ModelArgs, GemmaTextModel

__all__ = ["GemmaTextModel", "ModelArgs"]

gemma3_configs = {
    "debug": ModelArgs(
        vocab_size=262_144,  # Actual size from google/gemma-3-1b-pt tokenizer
        dim=1152,
        n_layers=4,
        n_heads=4,
        num_key_value_heads=1,
        head_dim=256,
        intermediate_size=6912,
        attn_types=["local_sliding", "local_sliding", "local_sliding", "local_sliding", "local_sliding", "global"],
        use_post_ffw_norm=True,
        use_pre_ffw_norm=True,
        sliding_window_size=512,
        rope_wave_length={
            "local_sliding": 10_000,
            "global": 1_000_000,
        },
        use_qk_norm=True,
        vision_config=None,
    ),
    "1B": ModelArgs(
        vocab_size=262_144,  # Actual size from google/gemma-3-1b-pt tokenizer
        dim=1152,
        n_layers=26,
        n_heads=4,
        num_key_value_heads=1,
        head_dim=256,
        intermediate_size=6912,
        attn_types=["local_sliding", "local_sliding", "local_sliding", "local_sliding", "local_sliding", "global"],
        use_post_ffw_norm=True,
        use_pre_ffw_norm=True,
        sliding_window_size=512,
        rope_wave_length={
            "local_sliding": 10_000,
            "global": 1_000_000,
        },
        use_qk_norm=True,
        vision_config=None,
    ),
    "4B": ModelArgs(
        vocab_size=262_208,
        dim=2560,
        n_layers=34,
        n_heads=8,
        num_key_value_heads=4,
        head_dim=256,
        intermediate_size=10240,
        attn_types=["local_sliding", "local_sliding", "local_sliding", "local_sliding", "local_sliding", "global"],
        use_post_ffw_norm=True,
        use_pre_ffw_norm=True,
        sliding_window_size=1024,
        rope_wave_length={
            "local_sliding": 10_000,
            "global": 1_000_000,
        },
        rope_scaling={"factor": 8.0},
        use_qk_norm=True,
        vision_config=None,
    ), 
    "12B": ModelArgs(
        vocab_size=262_208,
        dim=3840,
        n_layers=48,
        n_heads=16,
        num_key_value_heads=8,
        head_dim=256,
        intermediate_size=15360,
        attn_types=["local_sliding", "local_sliding", "local_sliding", "local_sliding", "local_sliding", "global"],
        use_post_ffw_norm=True,
        use_pre_ffw_norm=True,
        sliding_window_size=1024,
        rope_wave_length={
            "local_sliding": 10_000,
            "global": 1_000_000,
        },
        rope_scaling={"factor": 8.0},
        use_qk_norm=True,
        vision_config=None,
    ),
    "27B": ModelArgs(
        vocab_size=262_208,
        dim=5376,
        n_layers=62,
        n_heads=32,
        num_key_value_heads=16,
        head_dim=128,
        intermediate_size=21504,
        attn_types=["local_sliding", "local_sliding", "local_sliding", "local_sliding", "local_sliding", "global"],
        use_post_ffw_norm=True,
        use_pre_ffw_norm=True,
        query_pre_attn_scalar=168,
        sliding_window_size=1024,
        rope_wave_length={
            "local_sliding": 10_000,
            "global": 1_000_000,
        },
        rope_scaling={"factor": 8.0},
        use_qk_norm=True,
        vision_config=None,
    ),
} 
