from maester.models.glm4.model import ModelArgs, Glm4MoeTextModel
from maester.models.moe import MoEArgs

__all__ = ["Glm4MoeTextModel", "glm4_configs"]

glm4_configs = {
    "debug": ModelArgs(
        vocab_size=151_552,
        dim=512,                    
        intermediate_size=1024,       
        n_layers=4,                 
        n_heads=8,                  
        n_kv_heads=2,               
        head_dim=64,                
        hidden_act="silu",
        attention_dropout=0.0,
        max_position_embeddings=131_072,
        initializer_range=0.02,
        rms_norm_eps=1e-6,
        tie_word_embeddings=False,
        rope_theta=100000,
        rope_scaling=None,
        pad_token_id=0,
        eos_token_id=[2],
        bos_token_id=1,
        attention_bias=False,
        tied_embeddings=False,

        moe_args = MoEArgs(
            num_experts=8,          
            score_func="sigmoid",
            route_norm=True,
            top_k=2,                
            num_shared_experts=1,
            score_before_experts=False,
            use_grouped_mm=True,
        ),
        moe_intermediate_size=256,  
        first_k_dense_replace=1,
        use_qk_norm=False,
    ),
    "small": ModelArgs(
        vocab_size=151_552,
        dim=2048,
        intermediate_size=5376,
        n_layers=32,  
        n_heads=64,
        n_kv_heads=8,
        head_dim=128,
        hidden_act="silu",
        attention_dropout=0.0,
        max_position_embeddings=131_072,
        initializer_range=0.02,
        rms_norm_eps=1e-6,
        tie_word_embeddings=False,
        rope_theta=100000,
        rope_scaling=None,
        pad_token_id=0,
        eos_token_id=[2],
        bos_token_id=1,
        attention_bias=False,
        tied_embeddings=False,

        moe_args = MoEArgs(
            num_experts=64,
            score_func="sigmoid",
            route_norm=True,
            top_k=8,
            num_shared_experts=1,
            score_before_experts=False,
            use_grouped_mm=True,
        ),
        moe_intermediate_size=1024,
        first_k_dense_replace=1,
        use_qk_norm=False,
    ),
    "106B": ModelArgs(
        vocab_size=151_552,
        dim=4096,
        intermediate_size=10944,
        n_layers=46,
        n_heads=96,
        n_kv_heads=8,
        head_dim=128,
        hidden_act="silu",
        attention_dropout=0.0,
        max_position_embeddings=131_072,
        initializer_range=0.02,
        rms_norm_eps=1e-5,
        tie_word_embeddings=False,
        rope_theta=1000000,
        rope_scaling=None,
        pad_token_id=151329,
        eos_token_id=[151329, 151336, 151338],
        bos_token_id=1,
        attention_bias=True,
        tied_embeddings=False,
        moe_args = MoEArgs(
            num_experts=128,
            score_func="sigmoid",
            route_norm=True,
            top_k=8,
            num_shared_experts=1,
            score_before_experts=False,
            use_grouped_mm=True,
        ),
        moe_intermediate_size=1408,
        first_k_dense_replace=1,
        use_qk_norm=False,
    ), 
}