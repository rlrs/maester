from maester.models.glm4.model import ModelArgs, Glm4MoeTextModel

__all__ = ["Glm4MoeTextModel", "glm4_configs"]

glm4_configs = {
    "1B": ModelArgs(
        vocab_size=151_552,
        dim=2048,
        intermediate_size=5376,
        n_layers=16,    
        n_heads=16,
        n_kv_heads=4,
        head_dim=128,
        hidden_act="silu",
        attention_dropout=0.0,
        max_position_embeddings=131_072,
        initializer_range=0.02,
        rms_norm_eps=1e-6,
        use_cache=True,
        tie_word_embeddings=True,  # saves ~310M params
        rope_theta=10000.0,
        rope_scaling=None,
        pad_token_id=0,
        eos_token_id=[2],
        bos_token_id=1,
        attention_bias=True,
        tied_embeddings=False,
        # MoE-specific parameters for 1B model
        moe_intermediate_size=512,
        num_experts_per_tok=8,
        n_shared_experts=1,
        n_routed_experts=8,
        routed_scaling_factor=1.0,
        n_group=1,
        topk_group=1,
        first_k_dense_replace=1,
        norm_topk_prob=True,
        use_qk_norm=False,
    ),
    ".1B": ModelArgs(
        vocab_size=151_552,
        dim=512,
        intermediate_size=1536,
        n_layers=8,
        n_heads=8,
        n_kv_heads=4,
        head_dim=64,
        hidden_act="silu",
        attention_dropout=0.0,
        max_position_embeddings=8_192,
        initializer_range=0.02,
        rms_norm_eps=1e-6,
        use_cache=True,
        tie_word_embeddings=True,
        rope_theta=10000.0,
        rope_scaling=None,
        pad_token_id=0,
        eos_token_id=[2],
        bos_token_id=1,
        attention_bias=True,
        tied_embeddings=False,
        # MoE (smaller for debug)
        moe_intermediate_size=256,
        num_experts_per_tok=2,
        n_shared_experts=1,
        n_routed_experts=4,
        routed_scaling_factor=1.0,
        n_group=1,
        topk_group=1,
        first_k_dense_replace=2,
        norm_topk_prob=True,
        use_qk_norm=False,
    ),
}