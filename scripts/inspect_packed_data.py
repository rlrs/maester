#!/usr/bin/env python3
"""
Inspect and verify packed SFT data generated by pack_sft_data.py
"""

import argparse
import json
import numpy as np
import pandas as pd
from pathlib import Path
from collections import Counter
from transformers import AutoTokenizer, PreTrainedTokenizerFast
import os


def inspect_packed_data(data_path: str, tokenizer_name: str = None, num_samples: int = 10, decode_samples: int = 3):
    """Inspect packed SFT data for correctness."""
    
    # Load tokenizer if provided
    tokenizer = None
    if tokenizer_name:
        print(f"Loading tokenizer: {tokenizer_name}")
        if os.path.isfile(tokenizer_name):
            tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_name)
        else:
            tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        print(f"Special tokens: bos={repr(tokenizer.bos_token)}, eos={repr(tokenizer.eos_token)}, pad={repr(tokenizer.pad_token)}")
    
    print(f"\nLoading packed data from: {data_path}")
    df = pd.read_parquet(data_path)
    
    print("\n" + "="*60)
    print("DATASET OVERVIEW")
    print("="*60)
    print(f"Number of packs: {len(df)}")
    print(f"Columns: {df.columns.tolist()}")
    
    # Get sequence length from first pack
    seq_len = len(df.iloc[0]['input_ids'])
    print(f"Sequence length: {seq_len}")
    
    # Analyze packing statistics
    print("\n" + "="*60)
    print("PACKING STATISTICS")
    print("="*60)
    
    conversations_per_pack = []
    efficiency_per_pack = []
    
    for idx, row in df.iterrows():
        n_convs = len(row['conversation_ids'])
        conversations_per_pack.append(n_convs)
        
        attention_mask = np.array(row['attention_mask'])
        actual_tokens = attention_mask.sum()
        efficiency = actual_tokens / len(attention_mask)
        efficiency_per_pack.append(efficiency)
    
    print(f"Average conversations per pack: {np.mean(conversations_per_pack):.2f}")
    print(f"Min conversations per pack: {min(conversations_per_pack)}")
    print(f"Max conversations per pack: {max(conversations_per_pack)}")
    print(f"Distribution of conversations per pack:")
    conv_counts = Counter(conversations_per_pack)
    for count, freq in sorted(conv_counts.items()):
        print(f"  {count} conversations: {freq} packs ({freq/len(df)*100:.1f}%)")
    
    print(f"\nOverall packing efficiency: {np.mean(efficiency_per_pack):.2%}")
    print(f"Min pack efficiency: {min(efficiency_per_pack):.2%}")
    print(f"Max pack efficiency: {max(efficiency_per_pack):.2%}")
    
    # Verify all conversation IDs are unique
    print("\n" + "="*60)
    print("CONVERSATION ID VERIFICATION")
    print("="*60)
    
    all_conv_ids = []
    for idx, row in df.iterrows():
        all_conv_ids.extend(row['conversation_ids'])
    
    unique_conv_ids = set(all_conv_ids)
    print(f"Total conversation appearances: {len(all_conv_ids)}")
    print(f"Unique conversations: {len(unique_conv_ids)}")
    
    if len(all_conv_ids) != len(unique_conv_ids):
        print("WARNING: Some conversations appear multiple times!")
        duplicates = Counter(all_conv_ids)
        for conv_id, count in duplicates.most_common(10):
            if count > 1:
                print(f"  Conversation {conv_id}: appears {count} times")
    else:
        print("✓ All conversations appear exactly once")
    
    # Detailed inspection of sample packs
    print("\n" + "="*60)
    print(f"DETAILED INSPECTION (First {num_samples} packs)")
    print("="*60)
    
    for i in range(min(num_samples, len(df))):
        pack = df.iloc[i]
        print(f"\n--- Pack {pack['pack_id']} ---")
        
        # Basic info
        print(f"Conversations: {pack['conversation_ids']}")
        print(f"Number of conversations: {len(pack['conversation_ids'])}")
        
        # Verify lengths
        input_len = len(pack['input_ids'])
        label_len = len(pack['labels'])
        mask_len = len(pack['attention_mask'])
        
        if input_len != seq_len or label_len != seq_len or mask_len != seq_len:
            print(f"ERROR: Length mismatch! input={input_len}, labels={label_len}, mask={mask_len}, expected={seq_len}")
        else:
            print(f"✓ All arrays have correct length: {seq_len}")
        
        # Analyze boundaries
        print(f"Boundaries (conv_id, start, length):")
        total_boundary_tokens = 0
        for conv_id, start, length in pack['boundaries']:
            print(f"  Conv {conv_id}: starts at {start}, length {length}")
            total_boundary_tokens += length
            
            # Verify boundary is within attention mask
            attention_mask = np.array(pack['attention_mask'])
            if start + length <= len(attention_mask):
                mask_segment = attention_mask[start:start+length]
                if not np.all(mask_segment == 1):
                    print(f"    WARNING: Attention mask not all 1s for this segment")
            else:
                print(f"    ERROR: Boundary exceeds sequence length!")
        
        # Analyze attention mask
        attention_mask = np.array(pack['attention_mask'])
        actual_tokens = attention_mask.sum()
        padding_tokens = len(attention_mask) - actual_tokens
        
        print(f"Token usage:")
        print(f"  Actual tokens: {actual_tokens} ({actual_tokens/seq_len:.1%})")
        print(f"  Padding tokens: {padding_tokens} ({padding_tokens/seq_len:.1%})")
        print(f"  Boundary-reported tokens: {total_boundary_tokens}")
        
        if actual_tokens != total_boundary_tokens:
            print(f"  WARNING: Mismatch between attention mask ({actual_tokens}) and boundaries ({total_boundary_tokens})")
        
        # Check padding structure
        if padding_tokens > 0:
            # Find where padding starts
            first_zero = np.where(attention_mask == 0)[0]
            if len(first_zero) > 0:
                padding_start = first_zero[0]
                # Check if all padding is at the end
                if np.all(attention_mask[padding_start:] == 0):
                    print(f"  ✓ Padding is contiguous at the end (starts at position {padding_start})")
                else:
                    print(f"  WARNING: Non-contiguous padding detected!")
        
        # Check labels masking
        labels = np.array(pack['labels'])
        ignore_positions = (labels == -100).sum()
        print(f"Labels masking:")
        print(f"  Ignored positions: {ignore_positions} ({ignore_positions/seq_len:.1%})")
        
        # Sample some tokens to verify they're reasonable
        input_ids = np.array(pack['input_ids'])
        print(f"Sample input IDs (first 20): {input_ids[:20].tolist()}")
        if padding_tokens > 0:
            print(f"Sample padding IDs (last 20): {input_ids[-20:].tolist()}")
    
    # Decode packed sequences if tokenizer is available
    if tokenizer and decode_samples > 0:
        print("\n" + "="*60)
        print(f"DECODED SEQUENCES (First {decode_samples} packs)")
        print("="*60)
        
        for i in range(min(decode_samples, len(df))):
            pack = df.iloc[i]
            print(f"\n--- Pack {pack['pack_id']} (contains {len(pack['conversation_ids'])} conversations) ---")
            
            input_ids = np.array(pack['input_ids'])
            attention_mask = np.array(pack['attention_mask'])
            
            # Find where actual content ends (before padding)
            actual_length = attention_mask.sum()
            
            # Decode full sequence including special tokens
            full_text = tokenizer.decode(input_ids[:actual_length], skip_special_tokens=False)
            
            print("\nFull decoded text (with special tokens):")
            print("-" * 40)
            # Show first 1000 chars and last 500 chars to see boundaries
            if len(full_text) > 1500:
                print(full_text[:1000])
                print("\n... [middle portion truncated] ...\n")
                print(full_text[-500:])
            else:
                print(full_text)
            print("-" * 40)
            
            # Decode each conversation separately based on boundaries
            print("\nConversations by boundaries:")
            for conv_id, start, length in pack['boundaries']:
                conv_tokens = input_ids[start:start+length]
                conv_text = tokenizer.decode(conv_tokens, skip_special_tokens=False)
                print(f"\n  Conversation {conv_id} (tokens {start}:{start+length}):")
                print(f"  First 200 chars: {conv_text[:200]}")
                if len(conv_text) > 400:
                    print(f"  Last 200 chars: {conv_text[-200:]}")
                else:
                    print(f"  Full text: {conv_text}")
            
            # Check for special tokens between conversations
            print("\nSpecial token analysis:")
            bos_id = tokenizer.bos_token_id
            eos_id = tokenizer.eos_token_id
            pad_id = tokenizer.pad_token_id
            
            # Look for EOS/BOS tokens at conversation boundaries
            for j, (conv_id, start, length) in enumerate(pack['boundaries']):
                end = start + length
                
                # Check what's at the end of this conversation
                if end < len(input_ids):
                    end_tokens = input_ids[max(end-3, start):min(end+3, len(input_ids))].tolist()
                    end_text = tokenizer.decode(end_tokens, skip_special_tokens=False)
                    print(f"  Around boundary {j} (pos {end}): {end_tokens} => {repr(end_text)}")
                    
                    # Check if there's proper separation
                    if j < len(pack['boundaries']) - 1:
                        next_start = pack['boundaries'][j+1][1]
                        if next_start > end:
                            gap_tokens = input_ids[end:next_start].tolist()
                            gap_text = tokenizer.decode(gap_tokens, skip_special_tokens=False)
                            print(f"    Gap before next conv: {gap_tokens} => {repr(gap_text)}")
    
    # Check for potential issues
    print("\n" + "="*60)
    print("POTENTIAL ISSUES CHECK")
    print("="*60)
    
    issues = []
    
    # Check for empty packs
    empty_packs = sum(1 for eff in efficiency_per_pack if eff == 0)
    if empty_packs > 0:
        issues.append(f"{empty_packs} packs have 0% efficiency (all padding)")
    
    # Check for very low efficiency packs
    low_eff_threshold = 0.5
    low_eff_packs = sum(1 for eff in efficiency_per_pack if eff < low_eff_threshold)
    if low_eff_packs > 0:
        issues.append(f"{low_eff_packs} packs have efficiency below {low_eff_threshold:.0%}")
    
    # Check for single conversation packs (might indicate leftover handling)
    single_conv_packs = sum(1 for c in conversations_per_pack if c == 1)
    if single_conv_packs > len(df) * 0.1:  # More than 10% singleton packs
        issues.append(f"{single_conv_packs} singleton packs ({single_conv_packs/len(df)*100:.1f}% of total)")
    
    if issues:
        print("Found potential issues:")
        for issue in issues:
            print(f"  - {issue}")
    else:
        print("✓ No major issues detected")
    
    print("\n" + "="*60)
    print("SUMMARY")
    print("="*60)
    print(f"Total packs: {len(df)}")
    print(f"Total conversations packed: {len(unique_conv_ids)}")
    print(f"Average efficiency: {np.mean(efficiency_per_pack):.2%}")
    print(f"Average conversations per pack: {np.mean(conversations_per_pack):.2f}")
    
    return df


def main():
    parser = argparse.ArgumentParser(description="Inspect packed SFT data")
    parser.add_argument("data_path", type=str, help="Path to packed parquet file")
    parser.add_argument("--tokenizer", type=str, default=None,
                       help="Tokenizer name or path to decode sequences")
    parser.add_argument("--num_samples", type=int, default=5, 
                       help="Number of sample packs to inspect in detail")
    parser.add_argument("--decode_samples", type=int, default=3,
                       help="Number of packs to decode and display")
    
    args = parser.parse_args()
    
    if not Path(args.data_path).exists():
        print(f"Error: File not found: {args.data_path}")
        return
    
    inspect_packed_data(args.data_path, args.tokenizer, args.num_samples, args.decode_samples)


if __name__ == "__main__":
    main()